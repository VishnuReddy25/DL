{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPB/5CkEXYSr06SpDxKYDA2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnuReddy25/DL/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUxgVNsA8evE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare Dataset\n",
        "texts = [\n",
        "    \"I love this movie!\",\n",
        "    \"This is terrible.\",\n",
        "    \"I feel amazing today.\",\n",
        "    \"Worst experience ever.\",\n",
        "    \"Best product I bought!\",\n",
        "    \"I hate this.\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0]  # 1=Positive, 0=Negative\n",
        "\n",
        "# Split data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Initialize Tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize texts\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Step 3: Build Model\n",
        "bert_base = TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Input layers\n",
        "input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "# BERT outputs\n",
        "outputs = bert_base(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Classification head\n",
        "cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(cls_output)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "final_output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=final_output)\n",
        "\n",
        "# Step 4: Compile and Train\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x={'input_ids': train_encodings['input_ids'],\n",
        "       'attention_mask': train_encodings['attention_mask']},\n",
        "    y=np.array(train_labels),\n",
        "    validation_data=(\n",
        "        {'input_ids': val_encodings['input_ids'],\n",
        "         'attention_mask': val_encodings['attention_mask']},\n",
        "        np.array(val_labels)\n",
        "    ),\n",
        "    batch_size=2,\n",
        "    epochs=3\n",
        ")\n",
        "\n",
        "# Step 5: Prediction Function\n",
        "def predict_sentiment(text):\n",
        "    tokens = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
        "    prediction = model.predict({'input_ids': tokens['input_ids'],\n",
        "                               'attention_mask': tokens['attention_mask']})\n",
        "    return \"Positive\" if prediction[0][0] > 0.5 else \"Negative\"\n",
        "\n",
        "# Test predictions\n",
        "print(predict_sentiment(\"I absolutely loved it!\"))  # Should output: Positive\n",
        "print(predict_sentiment(\"This is the worst thing ever.\"))  # Should output: Negative"
      ]
    }
  ]
}